{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data where the description has been processed (lemmatized, stop word not removed, no stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "project_folder=\"C:/Users/muhammadkashifkhan/Documents/ASDS_2nd/Thesis/output_kashif/\"\n",
    "output_folder=project_folder+\"output\"\n",
    "\n",
    "df=pd.read_csv(output_folder+\"/\"+\"all_after_preprocessingLem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['description'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokeniser = RegexpTokenizer(r'\\w+')\n",
    "df[\"description\"]=df[\"description\"].apply(lambda x: tokeniser.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_nltk=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "real_estate_stopwords = [\n",
    "    #\"area\",\n",
    "    \"province\",\n",
    "    #\"location\",\n",
    "    #\"plot\",\n",
    "    ## common measurement\n",
    "    #\"hectare\",\n",
    "    #\"acre\",\n",
    "    #\"m2\",\n",
    "    #\"sq\",\n",
    "    #\"sale\",\n",
    "    #\"square\",\n",
    "    #\"meter\",\n",
    "    #\"metre\",\n",
    "    #\"feet\",\n",
    "    #\"foot\",\n",
    "    ## common rooms\n",
    "    #\"room\",\n",
    "    #\"bedroom\",\n",
    "    #\"bathroom\",\n",
    "    #\"bath\",\n",
    "    #\"washroom\",\n",
    "    #\"dining\",\n",
    "    #\"living\",\n",
    "    #\"kitchen\",\n",
    "    \n",
    "    #\"hallway\",\n",
    "    #\"corridor\",\n",
    "    \n",
    "    ## common occurance\n",
    "    \"extra\"\n",
    "    \n",
    "    ## type of the building\n",
    "    #\"apartment\",\n",
    "    #\"condo\",\n",
    "    #\"condominium\",\n",
    "    #\"home\",\n",
    "    #\"house\",\n",
    "    #\"unit\",\n",
    "    ## describe the appliances, too common\n",
    "    #\"stainless\",\n",
    "    #\"steel\",\n",
    "    ## common appliances\n",
    "    #\"washer\",\n",
    "    #\"dryer\",\n",
    "    #\"stove\",\n",
    "    #\"fridge\"\n",
    "    ]\n",
    "\n",
    "all_stop_words=stopwords_nltk+real_estate_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[\"description\"]=df[\"description\"].apply(lambda x: [word for word in x if word not in stopwords_nltk] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"]=df[\"description\"].apply(lambda x: \" \".join(word for word in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unique words\n",
    "uniqueWords = list(set(\" \".join(df['description']).split(\" \")))\n",
    "count = len(uniqueWords)\n",
    "print(\"Number of unique words is: \"+str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check n-gram, and word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "## This is the list of all the words in the description column\n",
    "totalWords = list(\" \".join(df['description']).split(\" \"))\n",
    "\n",
    "(pd.Series(nltk.ngrams(totalWords, 1)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "(pd.Series(nltk.ngrams(totalWords, 2)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(nltk.ngrams(totalWords, 3)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check 4-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(pd.Series(nltk.ngrams(totalWords, 4)).value_counts())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word cloud for all description data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(des for des in df.description)\n",
    "print (\"There are {} words in the combination of all description.\".format(len(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)#max_font_size=100, max_words=500,\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize = (50, 50))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word cloud for cheap 25% listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_5cheapest=df.sort_values(by=[\"price\"])[:516]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(des for des in df_5cheapest.description)\n",
    "print (\"There are {} words in the combination of all description.\".format(len(text)))\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)#max_font_size=100, max_words=500,\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize = (50, 50))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word cloud for middle price listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100middle=df.sort_values(by=[\"price\"])[5000:6001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(des for des in df_100middle.description)\n",
    "print (\"There are {} words in the combination of all description.\".format(len(text)))\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)#max_font_size=100, max_words=500,\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize = (100, 100))\n",
    "plt.imshow(wordcloud, interpolation='None')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word cloud for most expensive 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_5highest=df.sort_values(by=[\"price\"], ascending=False)[:516]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = \" \".join(des for des in df_5highest.description)\n",
    "print (\"There are {} words in the combination of all description.\".format(len(text)))\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)#max_font_size=100, max_words=500,\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize = (100, 100))\n",
    "plt.imshow(wordcloud, interpolation='None')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert description words to vectors using pre-trained google news word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the list of available pre-trained word vector models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = api.load(\"glove-twitter-200\")\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or, train the word2vec using description data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create list of sentences(sentence contain list of words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gensim\n",
    "corpus = df[\"description\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
    "               for i in range(0, len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)\n",
    "\n",
    "## detect bigrams and trigrams\n",
    "bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
    "                 delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
    "            delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(lst_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a Word2Vec model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gensim\n",
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "model = gensim.models.Word2Vec(lst_corpus, size=300, window=8, min_count=1, sg=1, iter=30)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save(output_folder+\"/w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec.load(output_folder+\"/w2v.model\")\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the word2vec vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(next(iter(w2v.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokeniser = RegexpTokenizer(r'\\w+')\n",
    "df[\"description\"]=df[\"description\"].apply(lambda x: tokeniser.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listt=df.description.head()\n",
    "for words in listt:\n",
    "    print(words)\n",
    "    for w in words:\n",
    "        print(w)\n",
    "        if w in w2v:\n",
    "            print(\"yes it is in word2vec\")\n",
    "        else:\n",
    "            print(\"no it is not\")\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get price and normalize price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "labels_price = pd.DataFrame(scaler.fit_transform(pd.DataFrame(df[\"price\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_price.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try use only description data to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## divide dataset into training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"description\"], labels_price, test_size=0.1, random_state=46) #random state=13 originally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the pipeline model with gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 4,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.01,\n",
    "          'loss': 'ls'}\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor(**params)#**params\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "graboo_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"gradient boosting\", reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert X_train to numpy, might be not necessary\n",
    "X_train=X_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check what the transformed vectors look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MEV=MeanEmbeddingVectorizer(w2v)\n",
    "transformed_X_train=MEV.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Long Short Term Memory, when only description data is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create model, required for KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def create_model(learn_rate=0.001, amsgrad=False, activation='relu', dropout_rate=0.0, neurons=50):\n",
    "    # create model\n",
    "    # The maximum number of words to be used. (most frequent)\n",
    "    #MAX_NB_WORDS = 50000\n",
    "    # embedding dimension\n",
    "    #EMBEDDING_DIM = 100\n",
    "    #model.add(Dense(1024, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    #model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    #model.add(LSTM(50))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation=activation)) #input_shape=(X_train.shape[1],), return_sequences = True\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons*2, activation=activation))\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1,activation ='sigmoid')) #, activation='sigmoid'\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learn_rate, amsgrad=amsgrad)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=10) #epochs=75, batch_size=10, verbose=10)\n",
    "# define the grid search parameters\n",
    "#optimizer = ['Adam'] \n",
    "batch_size = [10,20] # 5, \n",
    "epochs = [50, 75, 100] # ,\n",
    "learn_rate = [0.0001,0.001,0.01] #0.0001, , 0.01\n",
    "amsgrad = [False] # True,  #True,\n",
    "activation = ['relu', 'sigmoid']#, 'softplus'] #, 'sigmoid','softplus'] #, , 'softsign', 'hard_sigmoid', 'softmax', #, 'linear' \n",
    "dropout_rate = [0.1,0.2] #,0.3]#, 0.2] #0.0,, 0.3, 0.5 0.4, 0.2,, 0.3, 0.4, 0.5, 0.7\n",
    "neurons = [50, 100] #25, 50, 100, 150,300, 200\n",
    "\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, learn_rate=learn_rate, amsgrad=amsgrad, activation=activation, dropout_rate=dropout_rate, neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring=('r2', 'neg_root_mean_squared_error'), refit='r2')\n",
    "grid_result = grid.fit(transformed_X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Neural Network Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid.cv_results_['params']\n",
    "r2_scores=grid.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result = pd.DataFrame(grid.cv_results_)\n",
    "gd_result=gd_result[['param_batch_size','param_epochs', 'param_neurons','param_activation','param_learn_rate', 'param_dropout_rate', 'mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]\n",
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_NN_w2v_descriptiononly_5fold_corrected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Random Forest, when only description data is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "GBR = ensemble.RandomForestRegressor()\n",
    "\n",
    "\n",
    "parameters = {'bootstrap': [True, False],\n",
    "              'max_depth': [5, 10, 20, 30, None],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'n_estimators': [32, 64, 100, 500, 1000]}\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters,scoring=('r2', 'neg_root_mean_squared_error'),refit='r2', verbose=10, cv = 5, n_jobs=-1)\n",
    "grid_GBR.fit(transformed_X_train, y_train_flat)\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Gradient Boosting Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid_GBR.cv_results_['params']\n",
    "r2_scores=grid_GBR.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid_GBR.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "df_gridsearch_result = pd.DataFrame(grid_GBR.cv_results_)\n",
    "\n",
    "\n",
    "gd_result=df_gridsearch_result[['param_n_estimators','param_max_depth','param_max_features','param_bootstrap','mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_randomforest_w2v_selftrained_descriptiononly_5fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Gradient Boosting, when only description data is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "GBR = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "parameters = {'n_estimators' : [500,1000], # 100 removed\n",
    "              'max_depth'    : [4,6], # 3 removed\n",
    "                                       #'min_samples_split': [2, 5, 8],\n",
    "              'learning_rate': [0.01,0.02], # 0.005 removed\n",
    "                                     #'loss': ['ls'], # remove huber loss\n",
    "              'subsample'    : [1, 0.8] \n",
    "             }\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters,scoring=('r2', 'neg_root_mean_squared_error'),refit='r2', verbose=10, cv = 10, n_jobs=-1)\n",
    "grid_GBR.fit(transformed_X_train, y_train_flat)\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Gradient Boosting Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid_GBR.cv_results_['params']\n",
    "r2_scores=grid_GBR.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid_GBR.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "df_gridsearch_result = pd.DataFrame(grid_GBR.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result=df_gridsearch_result[['param_n_estimators','param_max_depth','param_learning_rate','param_subsample','mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]#'param_min_samples_split','param_loss',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_gradientboosting_W2V_self_trained_descriptiononly_5fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check how long it takes to finish the cross-validation\n",
    "import time\n",
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(graboo_w2v, X_train, y_train_flat, scoring=('r2', 'neg_root_mean_squared_error'), cv=10, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"RMSE training Score using cv: {:0.5f}\".format(scores['train_neg_root_mean_squared_error'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE test Score using cv: {:0.5f}\".format(scores['test_neg_root_mean_squared_error'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 training Score using cv: {:0.5f}\".format(scores['train_r2'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 test Score using cv: {:0.5f}\".format(scores['test_r2'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toc = time.perf_counter()\n",
    "print(f\"Finish cross validation in  {(toc - tic)/60:0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to use all features to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## description word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "MEV=MeanEmbeddingVectorizer(w2v)\n",
    "transformed_X_train=MEV.transform(df[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc=pd.DataFrame(transformed_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features=[\"bedroom\",\"baths\", 'Size', 'longitude', \"latitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X_num=df[numerical_features]\n",
    "X_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization for numerical data (exclude longitude and latitude) using MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# recaling the variables (both)\n",
    "X_num_columns = X_num.columns\n",
    "scaler = MinMaxScaler()\n",
    "X_num = scaler.fit_transform(X_num)\n",
    "\n",
    "# rename columns (since now its an np array)\n",
    "X_num = pd.DataFrame(X_num)\n",
    "X_num.columns = X_num_columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization for longitude and latitude sepeparately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num.drop([\"longitude\", \"latitude\"], axis=1)\n",
    "normed_long= df[\"longitude\"] *0.01\n",
    "normed_lat= df[\"latitude\"] *0.01\n",
    "X_num=pd.concat([X_num, normed_long, normed_lat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_features=['parkingAttachedGarage',\n",
    "       'parkingUnderground', 'parkingInsideEntry', 'parkingSurfaced',\n",
    "       'parkingOversize', 'parkingGravel', 'parkingGarage', 'parkingShared',\n",
    "       'parkingDetachedGarage', 'parkingCarport', 'parkingInterlocked',\n",
    "       'parkingVisitorParking','amenityClubhouse', 'amenityCarWash', 'amenityMusicRoom',\n",
    "       'amenityStorageLocker', 'amenitySauna', 'amenityPartyRoom',\n",
    "       'amenityRecreationCentre', 'amenityGuestSuite', 'amenityFurnished',\n",
    "       'amenityLaundryFacility', 'amenityExerciseCentre',\n",
    "       'amenityLaundryInSuite', 'amenitySecurity', 'amenityWhirlpool',\n",
    "       'efinishWood', 'efinishBrick', 'efinishHardboard', 'efinishWoodsiding',\n",
    "       'efinishLog', 'efinishMetal', 'efinishSteel', 'efinishStone',\n",
    "       'efinishWoodshingles', 'efinishStucco', 'efinishSiding',\n",
    "       'efinishConcrete', 'efinishShingles', 'efinishAluminumsiding',\n",
    "       'efinishCedarshingles', 'efinishVinyl', 'efinishVinylsiding',\n",
    "       'featurePetNotAllowed', 'AirportNearby',\n",
    "       'GolfNearby', 'MarinaNearby', 'ShoppingNearby', 'WaterNearby',\n",
    "       'WorshipPlaceNearby', 'RecreationNearby', 'PlaygroundNearby',\n",
    "       'PublicTransitNearby', 'ParkNearby', 'SchoolsNearby', 'HospitalNearby',\n",
    "       'HighwayNearby', 'SkiAreaNearby']\n",
    "\n",
    "X_boo=df[boolean_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert categorical data with string values into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_category=df[['location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## convert categorical data to numerical values\n",
    "cate_features=['location']\n",
    "for col in cate_features:\n",
    "    X_category[col] = X_category[col].astype('category')\n",
    "    X_category[col] = X_category[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# recaling the variables (both)\n",
    "X_category_columns = X_category.columns\n",
    "scaler = MinMaxScaler()\n",
    "X_category = scaler.fit_transform(X_category)\n",
    "\n",
    "# rename columns (since now its an np array)\n",
    "X_category = pd.DataFrame(X_category)\n",
    "X_category.columns = X_category_columns\n",
    "\n",
    "X_category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use numerical, boolean, categorical, and description data to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all = pd.concat([X_num, X_boo, X_category, df_desc], axis=1)\n",
    "X_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## divide dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, labels_price, test_size=0.1, random_state=13) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype(np.float32)\n",
    "X_test = np.asarray(X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Long Short Term Memory, when all features are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create model, required for KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def create_model(learn_rate=0.001, amsgrad=False, activation='relu', dropout_rate=0.0, neurons=50):\n",
    "    # create model\n",
    "    # The maximum number of words to be used. (most frequent)\n",
    "    #MAX_NB_WORDS = 50000\n",
    "    # embedding dimension\n",
    "    #EMBEDDING_DIM = 100\n",
    "    #model.add(Dense(1024, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    #model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    #model.add(LSTM(50))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation=activation)) #input_shape=(X_train.shape[1],), return_sequences = True\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons*2, activation=activation))\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1,activation ='sigmoid')) #, activation='sigmoid'\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learn_rate, amsgrad=amsgrad)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=10) #epochs=75, batch_size=10, verbose=10)\n",
    "# define the grid search parameters\n",
    "#optimizer = ['Adam'] \n",
    "batch_size = [10,20] # 5, 10,\n",
    "epochs = [50, 75, 100] # ,50, 75,\n",
    "learn_rate = [0.0001, 0.001, 0.01] #0.0001, , 0.01, 0.0001,0.001,\n",
    "amsgrad = [False] # True,  #True,\n",
    "activation = ['relu', 'sigmoid']#, 'softplus'] #, 'sigmoid','softplus'] #, , 'softsign', 'hard_sigmoid', 'softmax', #, 'linear' \n",
    "dropout_rate = [0.1, 0.2] #,0.2,0.3]#, 0.2] #0.0,, 0.3, 0.5 0.4, 0.2,, 0.3, 0.4, 0.5, 0.7\n",
    "neurons = [50, 100] #25, 50, 100, 150,300, 200\n",
    "\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, learn_rate=learn_rate, amsgrad=amsgrad, activation=activation, dropout_rate=dropout_rate, neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring=('r2', 'neg_root_mean_squared_error'), refit='r2')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Neural Network Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid.cv_results_['params']\n",
    "r2_scores=grid.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result = pd.DataFrame(grid.cv_results_)\n",
    "gd_result=gd_result[['param_batch_size','param_epochs', 'param_neurons','param_activation','param_learn_rate', 'param_dropout_rate', 'mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]\n",
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_NN_w2v_all_5fold_corrected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Random Forest, when all features are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "GBR = ensemble.RandomForestRegressor()\n",
    "\n",
    "\n",
    "parameters = {'bootstrap': [True, False],\n",
    "              'max_depth': [10, 20, 30, None],#5, \n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'n_estimators': [32, 64, 100, 500]}#, 1000\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters,scoring=('r2', 'neg_root_mean_squared_error'),refit='r2', verbose=10, cv = 5, n_jobs=-1)\n",
    "grid_GBR.fit(X_train, y_train_flat)\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Gradient Boosting Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid_GBR.cv_results_['params']\n",
    "r2_scores=grid_GBR.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid_GBR.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "df_gridsearch_result = pd.DataFrame(grid_GBR.cv_results_)\n",
    "\n",
    "\n",
    "gd_result=df_gridsearch_result[['param_n_estimators','param_max_depth','param_max_features','param_bootstrap','mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_randomforest_w2v_selftrained_all_5fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Gradient Boosting, when all features are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "GBR = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "parameters = {'n_estimators' : [500,1000], # 100 removed\n",
    "              'max_depth'    : [4,6], # 3 removed\n",
    "                                       #'min_samples_split': [2, 5, 8],\n",
    "              'learning_rate': [0.01,0.02], # 0.005 removed\n",
    "                                     #'loss': ['ls'], # remove huber loss\n",
    "              'subsample'    : [1, 0.8] \n",
    "             }\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters,scoring=('r2', 'neg_root_mean_squared_error'),refit='r2', verbose=10, cv = 5, n_jobs=-1)\n",
    "grid_GBR.fit(X_train, y_train_flat)\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Gradient Boosting Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid_GBR.cv_results_['params']\n",
    "r2_scores=grid_GBR.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid_GBR.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "df_gridsearch_result = pd.DataFrame(grid_GBR.cv_results_)\n",
    "\n",
    "\n",
    "gd_result=df_gridsearch_result[['param_n_estimators','param_max_depth','param_learning_rate','param_subsample','mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_gradientboosting_W2V_self_trained_all_5fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 4,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.01,\n",
    "          'loss': 'ls'}\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor(**params)#**params\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "reg.fit(X_train, y_train_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(reg, X_train, y_train_flat, scoring=('r2', 'neg_root_mean_squared_error'), cv=10, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE training Score using cv: {:0.5f}\".format(scores['train_neg_root_mean_squared_error'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"RMSE test Score using cv: {:0.5f}\".format(scores['test_neg_root_mean_squared_error'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 training Score using cv: {:0.5f}\".format(scores['train_r2'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"R2 test Score using cv: {:0.5f}\".format(scores['test_r2'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "toc = time.perf_counter()\n",
    "print(f\"Finish cross validation in  {(toc - tic)/60:0.4f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search using all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
