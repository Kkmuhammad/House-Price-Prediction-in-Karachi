{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e581b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fc6e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenized_dict = tokenizer.encode_plus(\n",
    "    \"hi my name is kashif\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=5,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_special_tokens_mask=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f664072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenized_text = torch.tensor(tokenized_dict[\"input_ids\"])\n",
    "with torch.no_grad():\n",
    "    embeddings = bert_model(torch.tensor(tokenized_text.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import torch\n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            bert_tokenizer,\n",
    "            bert_model,\n",
    "            max_length: int = 200,\n",
    "            embedding_func: Optional[Callable[[torch.tensor], torch.tensor]] = None,\n",
    "    ):\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.model = bert_model\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_func = embedding_func\n",
    "\n",
    "        if self.embedding_func is None:\n",
    "            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "    def _tokenize(self, text: str) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        # Tokenize the text with the provided tokenizer\n",
    "        tokenized_text = self.tokenizer.encode_plus(text,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    max_length=self.max_length\n",
    "                                                    )[\"input_ids\"]\n",
    "\n",
    "        # Create an attention mask telling BERT to use all words\n",
    "        attention_mask = [1] * len(tokenized_text)\n",
    "\n",
    "        # bert takes in a batch so we need to unsqueeze the rows\n",
    "        return (\n",
    "            torch.tensor(tokenized_text).unsqueeze(0),\n",
    "            torch.tensor(attention_mask).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def _tokenize_and_predict(self, text: str) -> torch.tensor:\n",
    "        tokenized, attention_mask = self._tokenize(text)\n",
    "\n",
    "        embeddings = self.model(tokenized, attention_mask)\n",
    "        return self.embedding_func(embeddings)\n",
    "\n",
    "    def transform(self, text: List[str]):\n",
    "        if isinstance(text, pd.Series):\n",
    "            text = text.tolist()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return torch.stack([self._tokenize_and_predict(string) for string in text])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder=\"C:/Users/muhammadkashifkhan/Documents/ASDS_2nd/Thesis/output_kashif/\"\n",
    "output_folder=project_folder+\"output_kashif\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2a367",
   "metadata": {},
   "source": [
    "## Normalize price and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv(output_folder+\"/\"+\"all_after_preprocessing6.csv\")#all_after_preprocessing6\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[\"price\"] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(df[\"price\"])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2b286",
   "metadata": {},
   "source": [
    "# split train dataset into train, validation and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['description'].isnull(), \"description\"] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['description'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3057c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['description'], df['price'], \n",
    "                                                                    random_state=13, \n",
    "                                                                    test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import ensemble\n",
    "\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 4,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.01,\n",
    "          'loss': 'ls'}\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "\n",
    "regressor = ensemble.GradientBoostingRegressor(**params)#**params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "bert_transformer = BertTransformer(tokenizer, bert_model)\n",
    "\n",
    "model_bert_gb = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", bert_transformer),\n",
    "        (\"regressor\", regressor),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca3dc54",
   "metadata": {},
   "source": [
    "## check what the transformed vectors look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train.isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9f355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BERT=BertTransformer(tokenizer, bert_model)\n",
    "transformed_X_train=BERT.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5246c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train = np.asarray(transformed_X_train).astype(np.float32)\n",
    "#X_test = np.asarray(X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aacf6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237c76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02699080",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Long Short Term Memory, when only description data is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dddba2f",
   "metadata": {},
   "source": [
    "#### Function to create model, required for KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed18b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def create_model(learn_rate=0.001, amsgrad=False, activation='relu', dropout_rate=0.0, neurons=50):\n",
    "    # create model\n",
    "    # The maximum number of words to be used. (most frequent)\n",
    "    #MAX_NB_WORDS = 50000\n",
    "    # embedding dimension\n",
    "    #EMBEDDING_DIM = 100\n",
    "    #model.add(Dense(1024, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    #model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    #model.add(LSTM(50))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation=activation)) #input_shape=(X_train.shape[1],), return_sequences = True\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons*2, activation=activation))\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid')) #\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learn_rate, amsgrad=amsgrad)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=10) #epochs=75, batch_size=10, verbose=10)\n",
    "# define the grid search parameters\n",
    "#optimizer = ['Adam'] \n",
    "batch_size = [10,20] # 5, \n",
    "epochs = [50, 75, 100] # ,\n",
    "learn_rate = [0.0001,0.001,0.01] #0.0001, , 0.01\n",
    "amsgrad = [False] # True,  #True,\n",
    "activation = ['relu', 'sigmoid']#, 'softplus'] #, 'sigmoid','softplus'] #, , 'softsign', 'hard_sigmoid', 'softmax', #, 'linear' \n",
    "dropout_rate = [0.1,0.2] #,0.3]#, 0.2] #0.0,, 0.3, 0.5 0.4, 0.2,, 0.3, 0.4, 0.5, 0.7\n",
    "neurons = [50, 100] #25, 50, 100, 150,300, 200\n",
    "\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, learn_rate=learn_rate, amsgrad=amsgrad, activation=activation, dropout_rate=dropout_rate, neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring=('r2', 'neg_root_mean_squared_error'), refit='r2')\n",
    "grid_result = grid.fit(transformed_X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf8981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Neural Network Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid.cv_results_['params']\n",
    "r2_scores=grid.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cbad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result = pd.DataFrame(grid.cv_results_)\n",
    "gd_result=gd_result[['param_batch_size','param_epochs', 'param_neurons','param_activation','param_learn_rate', 'param_dropout_rate', 'mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]\n",
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65366280",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_NN_BERT_descriptiononly_5fold_bound.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff8078",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Random Forest, when only description data is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec14756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "GBR = ensemble.RandomForestRegressor()\n",
    "\n",
    "\n",
    "parameters = {'bootstrap': [False], #True, \n",
    "              'max_depth': [30],  #5, 10, 20,, None\n",
    "              'max_features': ['sqrt'], #'auto',\n",
    "              'n_estimators': [500]} #32, 64, 100, 1000\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters,scoring=('r2', 'neg_root_mean_squared_error'),refit='r2', verbose=10, cv = 5, n_jobs=-1)\n",
    "grid_GBR.fit(transformed_X_train, y_train_flat)\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Gradient Boosting Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid_GBR.cv_results_['params']\n",
    "r2_scores=grid_GBR.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid_GBR.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "df_gridsearch_result = pd.DataFrame(grid_GBR.cv_results_)\n",
    "\n",
    "\n",
    "gd_result=df_gridsearch_result[['param_n_estimators','param_max_depth','param_max_features','param_bootstrap','mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49107c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_randomforest_BERT_descriptiononly_5fold_Lem.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0ef6c",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Gradient Boosting, using only description data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21866d7d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "GBR = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "parameters = {'n_estimators' : [500,1000], # 100 removed\n",
    "              'max_depth'    : [4,6], # 3 removed\n",
    "                                       #'min_samples_split': [2, 5, 8],\n",
    "              'learning_rate': [0.01,0.02], # 0.005 removed\n",
    "                                     #'loss': ['ls'], # remove huber loss\n",
    "              'subsample'    : [1, 0.8] \n",
    "             }\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters,scoring=('r2', 'neg_root_mean_squared_error'),refit='r2', verbose=10, cv = 5, n_jobs=-1)\n",
    "grid_GBR.fit(transformed_X_train, y_train_flat)\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Gradient Boosting Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid_GBR.cv_results_['params']\n",
    "r2_scores=grid_GBR.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid_GBR.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "df_gridsearch_result = pd.DataFrame(grid_GBR.cv_results_)\n",
    "\n",
    "\n",
    "gd_result=df_gridsearch_result[['param_n_estimators','param_max_depth','param_learning_rate','param_subsample','mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f377fa",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35937b7",
   "metadata": {},
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_gradientboosting_BERT_descriptiononly_5fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6829ac",
   "metadata": {},
   "source": [
    "## use only the description data to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419ca77f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Check how long it takes to finish the cross-validation\n",
    "import time\n",
    "tic = time.perf_counter()\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(regressor, transformed_X_train, y_train, scoring=('r2', 'neg_root_mean_squared_error'), cv=10, return_train_score=True)\n",
    "\n",
    "print(\"RMSE training Score using cv: {:0.5f}\".format(scores['train_neg_root_mean_squared_error'].mean() * -1))\n",
    "\n",
    "print(\"RMSE test Score using cv: {:0.5f}\".format(scores['test_neg_root_mean_squared_error'].mean() * -1))\n",
    "\n",
    "print(\"R2 training Score using cv: {:0.5f}\".format(scores['train_r2'].mean()))\n",
    "\n",
    "print(\"R2 test Score using cv: {:0.5f}\".format(scores['test_r2'].mean()))\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(f\"Finish cross validation in  {(toc - tic)/60:0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d6e7b",
   "metadata": {},
   "source": [
    "## Try to use all features to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14748ba",
   "metadata": {},
   "source": [
    "## description word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT=BertTransformer(tokenizer, bert_model)\n",
    "\n",
    "transformed_X_train=BERT.transform(df[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceda49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train=transformed_X_train.numpy()\n",
    "df_desc=pd.DataFrame(transformed_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1204bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136de641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99310ca",
   "metadata": {},
   "source": [
    "## numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features=[\"bedrooms\",\"baths\", 'size', 'longitude', \"latitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X_num=df[numerical_features]\n",
    "X_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650355ac",
   "metadata": {},
   "source": [
    "## Normalization for numerical data (exclude longitude and latitude) using MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# recaling the variables (both)\n",
    "X_num_columns = X_num.columns\n",
    "scaler = MinMaxScaler()\n",
    "X_num = scaler.fit_transform(X_num)\n",
    "\n",
    "# rename columns (since now its an np array)\n",
    "X_num = pd.DataFrame(X_num)\n",
    "X_num.columns = X_num_columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08ae5b",
   "metadata": {},
   "source": [
    "## Normalization for longitude and latitude sepeparately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num.drop([\"longitude\", \"latitude\"], axis=1)\n",
    "normed_long= df[\"longitude\"] *0.01\n",
    "normed_lat= df[\"latitude\"] *0.01\n",
    "X_num=pd.concat([X_num, normed_long, normed_lat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d74bc3",
   "metadata": {},
   "source": [
    "## Boolean features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f8ee5",
   "metadata": {},
   "source": [
    "## Convert categorical data with string values into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_category=df[['location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e9834a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## convert categorical data to numerical values\n",
    "cate_features=['location']\n",
    "for col in cate_features:\n",
    "    X_category[col] = X_category[col].astype('category')\n",
    "    X_category[col] = X_category[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd993ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc169b56",
   "metadata": {},
   "source": [
    "## Normalize the categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a040b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# recaling the variables (both)\n",
    "X_category_columns = X_category.columns\n",
    "scaler = MinMaxScaler()\n",
    "X_category = scaler.fit_transform(X_category)\n",
    "\n",
    "# rename columns (since now its an np array)\n",
    "X_category = pd.DataFrame(X_category)\n",
    "X_category.columns = X_category_columns\n",
    "\n",
    "X_category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23751ba",
   "metadata": {},
   "source": [
    "## Use numerical, categorical, and description data to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e51b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all = pd.concat([X_num, X_category, df_desc], axis=1)\n",
    "X_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc82e7c",
   "metadata": {},
   "source": [
    "## divide dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, df[\"price\"], test_size=0.1, random_state=13) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype(np.float32)\n",
    "#X_test = np.asarray(X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3a28be",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Long Short Term Memory, when only description data is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacdc8c8",
   "metadata": {},
   "source": [
    "#### Function to create model, required for KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343aeb64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def create_model(learn_rate=0.001, amsgrad=False, activation='relu', dropout_rate=0.0, neurons=50):\n",
    "    # create model\n",
    "    # The maximum number of words to be used. (most frequent)\n",
    "    #MAX_NB_WORDS = 50000\n",
    "    # embedding dimension\n",
    "    #EMBEDDING_DIM = 100\n",
    "    #model.add(Dense(1024, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    #model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    #model.add(LSTM(50))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation=activation)) #input_shape=(X_train.shape[1],), return_sequences = True\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons*2, activation=activation))\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid')) #\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learn_rate, amsgrad=amsgrad)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=10) #epochs=75, batch_size=10, verbose=10)\n",
    "# define the grid search parameters\n",
    "#optimizer = ['Adam'] \n",
    "batch_size = [10,20] # 5, \n",
    "epochs = [50, 75, 100] # ,\n",
    "learn_rate = [0.0001,0.001,0.01] #0.0001, , 0.01\n",
    "amsgrad = [False] # True,  #True,\n",
    "activation = ['relu', 'sigmoid']#, 'softplus'] #, 'sigmoid','softplus'] #, , 'softsign', 'hard_sigmoid', 'softmax', #, 'linear' \n",
    "dropout_rate = [0.1,0.2] #,0.3]#, 0.2] #0.0,, 0.3, 0.5 0.4, 0.2,, 0.3, 0.4, 0.5, 0.7\n",
    "neurons = [50, 100] #25, 50, 100, 150,300, 200\n",
    "\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, learn_rate=learn_rate, amsgrad=amsgrad, activation=activation, dropout_rate=dropout_rate, neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring=('r2', 'neg_root_mean_squared_error'), refit='r2')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac00df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Neural Network Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid.cv_results_['params']\n",
    "r2_scores=grid.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb451ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result = pd.DataFrame(grid.cv_results_)\n",
    "gd_result=gd_result[['param_batch_size','param_epochs', 'param_neurons','param_activation','param_learn_rate', 'param_dropout_rate', 'mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]\n",
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_NN_BERT_all_5fold_bound.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd08c7",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Random Forest, when all features are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefeec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "GBR = ensemble.RandomForestRegressor()\n",
    "\n",
    "\n",
    "parameters = {'bootstrap': [True, False],\n",
    "              'max_depth': [10, 20, 30, None], #5\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'n_estimators': [32, 64, 100, 500]} #1000\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters,scoring=('r2', 'neg_root_mean_squared_error'),refit='r2', verbose=10, cv = 5, n_jobs=-1)\n",
    "grid_GBR.fit(X_train, y_train_flat)\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Gradient Boosting Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid_GBR.cv_results_['params']\n",
    "r2_scores=grid_GBR.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid_GBR.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "df_gridsearch_result = pd.DataFrame(grid_GBR.cv_results_)\n",
    "\n",
    "\n",
    "gd_result=df_gridsearch_result[['param_n_estimators','param_max_depth','param_max_features','param_bootstrap','mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8979c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_randomforest_BERT_all_5fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7feb2",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Gradient Boosting, using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba187509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "GBR = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "parameters = {'n_estimators' : [500,1000], # 100 removed\n",
    "              'max_depth'    : [4,6], # 3 removed\n",
    "                                       #'min_samples_split': [2, 5, 8],\n",
    "              'learning_rate': [0.01,0.02], # 0.005 removed\n",
    "                                     #'loss': ['ls'], # remove huber loss\n",
    "              'subsample'    : [1, 0.8] \n",
    "             }\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_GBR = GridSearchCV(estimator=GBR, param_grid = parameters,scoring=('r2', 'neg_root_mean_squared_error'),refit='r2', verbose=10, cv = 5) #, n_jobs=-1\n",
    "grid_GBR.fit(X_train, y_train_flat)\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Gradient Boosting Grid Search Results\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "print(\"\\n The best r2 score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"All Results:\") \n",
    "\n",
    "param_com=grid_GBR.cv_results_['params']\n",
    "r2_scores=grid_GBR.cv_results_['mean_test_r2']\n",
    "rmse_scores=grid_GBR.cv_results_['mean_test_neg_root_mean_squared_error']\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "index=0\n",
    "for item in param_com:\n",
    "    print(\"parameter combinations:\"+str(item))\n",
    "    print(\"\\n\")\n",
    "    print(\"test r2 score:\"+str(r2_scores[index]))\n",
    "    print(\"\\n\")\n",
    "    print(\"test RMSE score:\"+str(rmse_scores[index]*-1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "df_gridsearch_result = pd.DataFrame(grid_GBR.cv_results_)\n",
    "\n",
    "\n",
    "gd_result=df_gridsearch_result[['param_n_estimators','param_max_depth','param_learning_rate','param_subsample','mean_test_r2','mean_test_neg_root_mean_squared_error'  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a696a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_result=gd_result.sort_values(by=['mean_test_r2', 'mean_test_neg_root_mean_squared_error'], ascending=False)\n",
    "gd_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2affe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result.to_csv(output_folder+\"/\"+\"gridsearch_gradientboosting_BERT_all_5fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50556b",
   "metadata": {},
   "source": [
    "## create gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87501bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 4,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.01,\n",
    "          'loss': 'ls'}\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor(**params)#**params\n",
    "\n",
    "y_train_flat=np.ravel(y_train)\n",
    "reg.fit(X_train, y_train_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa293fce",
   "metadata": {},
   "source": [
    "## do cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ee9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(reg, X_train, y_train_flat, scoring=('r2', 'neg_root_mean_squared_error'), cv=10, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e63a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE training Score using cv: {:0.5f}\".format(scores['train_neg_root_mean_squared_error'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d70e93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"RMSE test Score using cv: {:0.5f}\".format(scores['test_neg_root_mean_squared_error'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 training Score using cv: {:0.5f}\".format(scores['train_r2'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d2547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"R2 test Score using cv: {:0.5f}\".format(scores['test_r2'].mean() * -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1d03a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "toc = time.perf_counter()\n",
    "print(f\"Finish cross validation in  {(toc - tic)/60:0.4f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
